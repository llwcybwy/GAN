{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> \n",
    "<b>\n",
    "Picture Monetization\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "The goal with this project is to implement a Generative Adversarial Network between two dataset: a dataset consisting of 7038 pictures (`photo`) and a dataset consisting of 300 Monet paintings (`monet`). To achieve this, we will implement two __generators__, capable of taking images from one dataset and outputing an image following the same distribution as the other dataset, and one __discriminator__, which will be trained to determine whether an image is fake or real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, install the following packages by running the code below.\n",
    "\n",
    "`pip install ipykernel numpy torch albumentations pillow tqdm torchvision transformers absl-py`\n",
    "\n",
    "Run the following imports to load all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jocke\\OneDrive\\Skrivbord\\Skola\\Delft_VT25\\ML Bayes - EE4685\\GAN\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Force the spawn method (this is required on Windows)\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Config\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "The configurations for the GAN. Some variables such as the directories may have to be tweaked to fit your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Prefer to work on GPU if available\n",
    "TRAIN_DIR = \"Data/train\"\n",
    "VAL_DIR = \"Data/val\"\n",
    "BATCH_SIZE = 1 # i.e. SGD\n",
    "LEARNING_RATE = 1e-5\n",
    "LAMBDA_IDENTITY = 5 # 0.0 # weight for identity loss (photo->photo, Monet->Monet)\n",
    "LAMBDA_CYCLE = 10 # weight for cycle loss (photo->Monet, Monet->photo)\n",
    "NUM_WORKERS = 0 # may need to change to 0 if no workers are available\n",
    "NUM_EPOCHS = 10\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN_P = \"genp.pth.tar\" # photos\n",
    "CHECKPOINT_GEN_M = \"genm.pth.tar\" # monet\n",
    "CHECKPOINT_CRITIC_P = \"criticp.pth.tar\"\n",
    "CHECKPOINT_CRITIC_M = \"criticm.pth.tar\"\n",
    "\n",
    "transforms = A.Compose( # Note that we only expand the dataset by flipping, and not by altering the colors or brightnesses (since these are intregral to the photos and Monet )\n",
    "    [\n",
    "        A.Resize(width=256, height=256),\n",
    "        A.HorizontalFlip(p=0.5), # Double dataset by flipping images\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    additional_targets={\"image0\": \"image\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Dataset\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We define the class of the dataset holding the photos and the Monet pictures, along with its generator, length, and item retrieval function. The retrieval finds the photo and Monet picture corresponding to the given index, finds the images from their respective paths, applies the given transformation on them (if any), and returns the two pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoMonetDataset(Dataset):\n",
    "    def __init__(self, root_photo, root_monet, transform=None):\n",
    "        self.root_photo = root_photo # dir to photos\n",
    "        self.root_monet = root_monet # dir to monet pictures\n",
    "        self.transform = transform\n",
    "\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.length_dataset = max(self.photo_len, self.monet_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        photo_img = self.photo_images[index % self.photo_len ] # preventing index errors\n",
    "        monet_img = self.monet_images[index % self.monet_len ]\n",
    "\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        print(f\"Trying to load: {monet_path}\")\n",
    "        if not os.path.exists(monet_path):\n",
    "            print(f\"âŒ ERROR: File not found -> {monet_path}\")\n",
    "        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=photo_img, image0=monet_img)\n",
    "            photo_img = augmentations[\"image\"]\n",
    "            monet_img = augmentations[\"image0\"]\n",
    "\n",
    "        return photo_img, monet_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Models\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "The GAN essentially consists of two models: a discriminator, which decides if an image is fake or not, and a generator, which generates fake images by converting an image from distribution (e.g. photo) to another (e.g. Monet painting). They work together to correctly train the model, see the below section on __Training__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "<b>\n",
    "Discriminator model\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "__Block__: Shorthand for a convolution block, consisting of a 2d convolution layer, a normalization, and a ReLU. Used to define the discriminatory model.\n",
    "\n",
    "__Discriminator__: Classifies images into \"Real\" and \"Fake\". It consists of blocks of iteratively smaller size, finally culminating in a one-dimensional output (0 is \"fake\" and 1 is \"true\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):   # inheriting from nn. Module\n",
    "    def __init__(self, in_channels, out_channels, stride ):\n",
    "        super().__init__()                          # is a way to call the constructor of a parent class in Python. It ensures that the parent class (nn.Module in PyTorch) is properly initialized when a child class is created.\n",
    "        self.conv =nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,4,stride,1,bias=True,padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64,128,256,512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(nn.Conv2d(in_channels,features[0],kernel_size=4,stride=2,padding=1, padding_mode=\"reflect\"),nn.LeakyReLU(0.2))\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride = 1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels,1,kernel_size=4,stride=1,padding=1, padding_mode=\"reflect\")) # output has dimension 1\n",
    "        self.model = nn.Sequential(*layers) # unwrapping the list\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x)) # normalize the output between 0 and 1\n",
    "\n",
    "    def test():\n",
    "        x = torch.randn((5, 3, 256, 256))\n",
    "        model = Discriminator(in_channels=3)\n",
    "        preds = model(x)\n",
    "        print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# Run this block to test the discriminator\n",
    "Discriminator.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "<b>\n",
    "Generator model\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We wish to remove all \"unimportant features\" from the input to a bottleneck in the middle (downsampling), and then upscale it to the desired distribution, but keeping the most important features (upsampling). \n",
    "\n",
    "__ConvBlock__: Conv2d + ReLU if `down=True`, else the transposed (i.e., the same but in the other direction).\n",
    "\n",
    "__ResidualBlock__: Consists of two ConvBlock(down=True). Forward has a residual to avoid 0 gradients in deep networks.\n",
    "\n",
    "__Generator__: Consists of an initial convolution that processes the image input, followed by two down ConvBlocks boiling it down to its most important features, followed by some ResidualBlocks that process the features (without risking gradient losses due to deep networks). Finally, two up ConvBlocks are applied and a last convolution (i.e., the same as the beginning, without ReLU but using `tanh` in the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):    # Down and upsampling\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs) :#key word arguments\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels,out_channels,**kwargs), # either Conv2d (id down) or its transpose\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block= nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1, stride=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x+ self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "           [ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1)]\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range (num_residuals) ]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [ConvBlock(num_features*4, num_features*2,down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "             ConvBlock(num_features*2, num_features*1,down=False, kernel_size=3, stride=2, padding=1, output_padding=1)]\n",
    "        )\n",
    "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7,stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward (self,x):\n",
    "        x= self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x= layer(x)\n",
    "        x=self.residual_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x= layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "def generator_test():\n",
    "    img_channels = 3\n",
    "    img_size =256\n",
    "    x= torch.randn((2, img_channels, img_size, img_size))\n",
    "    gen = Generator(img_channels,64,9 )\n",
    "    print(gen(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Run to test the generator\n",
    "generator_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Utils\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Utility functions used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Training\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Here, we are first training the discriminant and then the generator. We are using the generator to get the loss of the discriminant, and the discriminant to train the generator. Note that for low batch sizes (e.g. 1), the difference between the generator/discriminant before and after the weight updating is quite small, which might be good since we ideally would like to update them \"at the same time\". This is of course impossible, but by using small batch sizes, we approximate this better.\n",
    "\n",
    "__Training of the discriminant__\n",
    "\n",
    "We first try our function on a true photo, then generate a fake photo from the Monet data and try our discriminant on this photo. We then compare the outputed values with the correct ones (1 and 0 respectively) using mse, and let the loss be the sum of the two. We then do the same for Monet and let the total loss be the average of the two losses. Finally, we update the weights based on the losses. This way, we train the discriminant to recognize both fake and true data using the generator. As the generator improves, so will the discriminant.\n",
    "\n",
    "__Training the generator__\n",
    "\n",
    "We wish to improve the generator in three regards:\n",
    "\n",
    "1. __Adverserial__: We want the generator to be able to fool the discriminant. Since the discriminant is improved in the previous iteration, we thus want to generate data and change the weights in the generator such that the discrimant believes it to be true, hence why we take the mse of the fake data wrt 1. Note that by \"moving away\" from the discriminator, we do not train a generator which can create images that the discriminator is tailored to recognize and converge pretty quickly, but we instead capitalize from our improved discriminator and use it to iteratively improve towards data which more closely resembles the target distribution.\n",
    "2. __Cycle__: An image is converted to the other distribution, and then back again to the first distribution. It is then compared with the original image using the $L^1$ norm. By doing this, we hope to train the CycleGAN to only make necessary changes to images, and for the two used generators to identify the same features to change.\n",
    "3. __Identity__: To strengthen our model's capacity to generate images of the target distribution, we input an image of that distribution and similar to above take the $L^1$-loss.\n",
    "\n",
    "We combine all of these losses by summing them together, along with a weight factor to the cycle `LAMBDA_CYCLE` and identity losses `LAMBDA_IDENTITY` (and by extension also to the adversarial loss since lowering the weights of the others de-facto raises that of the adversarial one, and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(disc_P, disc_M, gen_P, gen_M, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler, epoch):\n",
    "    loop = tqdm(loader, leave=True)    # progress bar\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        print(f\"Entering iteration {idx}\")\n",
    "        photo = photo.to(DEVICE)\n",
    "        monet = monet.to(DEVICE)\n",
    "\n",
    "        # Train Discriminators P and M.\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            fake_Photo = gen_P(monet) # generate a fake photo based off a Monet picture\n",
    "            D_P_real = disc_P(photo) # take a real photo and see if it is real or not\n",
    "            D_P_fake = disc_P(fake_Photo.detach()) # check whether the fake photo is classified as real or not\n",
    "            D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "            D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_fake))\n",
    "            D_P_loss = D_P_fake_loss+D_P_real_loss # loss based on how well it classified true and fake images\n",
    "\n",
    "            fake_Monet = gen_M(photo)\n",
    "            D_M_real = disc_P(monet)\n",
    "            D_M_fake = disc_P(fake_Monet.detach())\n",
    "            D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n",
    "            D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n",
    "            D_M_loss = D_M_fake_loss + D_M_real_loss\n",
    "\n",
    "            D_loss = (D_P_loss+D_M_loss)/2 # loss as average of Monet and photo\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update() # Update the discriminator based on the loss\n",
    "\n",
    "        # Train generators P and M\n",
    "        with ((torch.amp.autocast('cuda'))):\n",
    "            # Adverserial loss\n",
    "            D_P_fake = disc_P(fake_Photo) # whether the photo is fake or not (after updating)\n",
    "            D_M_fake = disc_M(fake_Monet)\n",
    "            Loss_G_M = mse(D_M_fake, torch.ones_like(D_M_fake))\n",
    "            Loss_G_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "            # Cycle loss\n",
    "            cycle_monet = gen_M(fake_Photo) # make a Monet out of a photo\n",
    "            cycle_photo = gen_P(fake_Monet)\n",
    "            cycle_monet_loss = l1(cycle_monet, monet)\n",
    "            cycle_photo_loss = l1(cycle_photo,photo)\n",
    "            # Identitiy loss\n",
    "            identity_photo= gen_P(photo) # generate photo out of a photo\n",
    "            identity_monet= gen_M(monet)\n",
    "            identity_monet_loss= l1(identity_photo,photo)\n",
    "            identity_photo_loss= l1(identity_monet,monet)\n",
    "            # Add all together\n",
    "            G_loss = (Loss_G_M+Loss_G_P\n",
    "            + cycle_monet_loss* LAMBDA_CYCLE\n",
    "            + cycle_photo_loss* LAMBDA_CYCLE\n",
    "            + identity_monet_loss * LAMBDA_IDENTITY\n",
    "            + identity_photo_loss * LAMBDA_IDENTITY) # use everything in loss, with weights\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update() # update based on total loss\n",
    "\n",
    "        if idx % 20 == 0:\n",
    "            save_image(fake_Photo * 0.5 + 0.5, f\"saved_images/photo_{epoch}_{idx}.png\")\n",
    "            save_image(fake_Monet * 0.5 + 0.5, f\"saved_images/monet_{epoch}_{idx}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fn(disc_P, disc_M, gen_P, gen_M, loader, l1, mse, epoch):\n",
    "    # Set models to evaluation mode\n",
    "    disc_P.eval()\n",
    "    disc_M.eval()\n",
    "    gen_P.eval()\n",
    "    gen_M.eval()\n",
    "\n",
    "    total_D_loss = 0.0\n",
    "    total_G_loss = 0.0\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    # Disable gradient computations for validation\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(loader, leave=True)\n",
    "        for idx, (monet, photo) in enumerate(loop):\n",
    "            photo = photo.to(DEVICE)\n",
    "            monet = monet.to(DEVICE)\n",
    "            # --------------------\n",
    "            #  Discriminator Loss\n",
    "            # --------------------\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                fake_Photo = gen_P(monet)\n",
    "                D_P_real = disc_P(photo)\n",
    "                D_P_fake = disc_P(fake_Photo)\n",
    "                D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "                D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_fake))\n",
    "                D_P_loss = D_P_real_loss + D_P_fake_loss\n",
    "\n",
    "                fake_Monet = gen_M(photo)\n",
    "                D_M_real = disc_M(monet)\n",
    "                D_M_fake = disc_M(fake_Monet)\n",
    "                D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n",
    "                D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n",
    "                D_M_loss = D_M_real_loss + D_M_fake_loss\n",
    "\n",
    "                D_loss = (D_P_loss + D_M_loss) / 2\n",
    "\n",
    "            # --------------------\n",
    "            #  Generator Loss\n",
    "            # --------------------\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Re-compute for generators (if needed for validation metrics)\n",
    "                D_P_fake = disc_P(fake_Photo)\n",
    "                D_M_fake = disc_M(fake_Monet)\n",
    "                Loss_G_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "                Loss_G_M = mse(D_M_fake, torch.ones_like(D_M_fake))\n",
    "\n",
    "                # Cycle consistency\n",
    "                cycle_monet = gen_M(fake_Photo)\n",
    "                cycle_photo = gen_P(fake_Monet)\n",
    "                cycle_monet_loss = l1(cycle_monet, monet)\n",
    "                cycle_photo_loss = l1(cycle_photo, photo)\n",
    "\n",
    "                # Identity loss\n",
    "                identity_photo = gen_P(photo)\n",
    "                identity_monet = gen_M(monet)\n",
    "                identity_monet_loss = l1(identity_photo, photo)\n",
    "                identity_photo_loss = l1(identity_monet, monet)\n",
    "\n",
    "                G_loss = (Loss_G_P + Loss_G_M +\n",
    "                          cycle_monet_loss * LAMBDA_CYCLE +\n",
    "                          cycle_photo_loss * LAMBDA_CYCLE +\n",
    "                          identity_monet_loss * LAMBDA_IDENTITY +\n",
    "                          identity_photo_loss * LAMBDA_IDENTITY)\n",
    "            # --------------------\n",
    "            #  CMMD\n",
    "            # --------------------\n",
    "            with torch.no_grad():\n",
    "                fake_Monet = fake_Monet.squeeze(0).cpu()  # Remove batch dimension\n",
    "                fake_Photo = fake_Photo.squeeze(0).cpu()\n",
    "                # Convert back to image format\n",
    "                save_image_Monet = transforms.ToPILImage()(fake_Monet * 0.5 + 0.5)  # Denormalize\n",
    "                save_image_Photo = transforms.ToPILImage()(fake_Photo * 0.5 + 0.5)\n",
    "                save_image_Monet.save(f'Output/Monet/{epoch}/{idx}.jpg')\n",
    "                save_image_Photo.save(f'Output/Photo/{epoch}/{idx}.jpg')\n",
    "            total_D_loss += D_loss.item()\n",
    "            total_G_loss += G_loss.item()\n",
    "\n",
    "    avg_D_loss = total_D_loss / num_batches\n",
    "    avg_G_loss = total_G_loss / num_batches\n",
    "    # print(f\"Epoch {epoch} | Validation D Loss: {avg_D_loss:.4f}, G Loss: {avg_G_loss:.4f}\")\n",
    "\n",
    "    # Set models back to training mode\n",
    "    disc_P.train()\n",
    "    disc_M.train()\n",
    "    gen_P.train()\n",
    "    gen_M.train()\n",
    "\n",
    "    return avg_D_loss, avg_G_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "CMMD code\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "CMMD is used to evaluate the performance of the code. The code was taken from __[github](https://github.com/sayakpaul/cmmd-pytorch)__, written by Sayak Paul and Agneet Chatterjee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Memory-efficient MMD implementation in JAX.\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# The bandwidth parameter for the Gaussian RBF kernel. See the paper for more\n",
    "# details.\n",
    "_SIGMA = 10\n",
    "# The following is used to make the metric more human readable. See the paper\n",
    "# for more details.\n",
    "_SCALE = 1000\n",
    "\n",
    "\n",
    "def mmd(x, y):\n",
    "    \"\"\"Memory-efficient MMD implementation in JAX.\n",
    "\n",
    "    This implements the minimum-variance/biased version of the estimator described\n",
    "    in Eq.(5) of\n",
    "    https://jmlr.csail.mit.edu/papers/volume13/gretton12a/gretton12a.pdf.\n",
    "    As described in Lemma 6's proof in that paper, the unbiased estimate and the\n",
    "    minimum-variance estimate for MMD are almost identical.\n",
    "\n",
    "    Note that the first invocation of this function will be considerably slow due\n",
    "    to JAX JIT compilation.\n",
    "\n",
    "    Args:\n",
    "      x: The first set of embeddings of shape (n, embedding_dim).\n",
    "      y: The second set of embeddings of shape (n, embedding_dim).\n",
    "\n",
    "    Returns:\n",
    "      The MMD distance between x and y embedding sets.\n",
    "    \"\"\"\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    x_sqnorms = torch.diag(torch.matmul(x, x.T))\n",
    "    y_sqnorms = torch.diag(torch.matmul(y, y.T))\n",
    "\n",
    "    gamma = 1 / (2 * _SIGMA**2)\n",
    "    k_xx = torch.mean(\n",
    "        torch.exp(-gamma * (-2 * torch.matmul(x, x.T) + torch.unsqueeze(x_sqnorms, 1) + torch.unsqueeze(x_sqnorms, 0)))\n",
    "    )\n",
    "    k_xy = torch.mean(\n",
    "        torch.exp(-gamma * (-2 * torch.matmul(x, y.T) + torch.unsqueeze(x_sqnorms, 1) + torch.unsqueeze(y_sqnorms, 0)))\n",
    "    )\n",
    "    k_yy = torch.mean(\n",
    "        torch.exp(-gamma * (-2 * torch.matmul(y, y.T) + torch.unsqueeze(y_sqnorms, 1) + torch.unsqueeze(y_sqnorms, 0)))\n",
    "    )\n",
    "\n",
    "    return _SCALE * (k_xx + k_yy - 2 * k_xy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Embedding models used in the CMMD calculation.\"\"\"\n",
    "\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "_CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14-336\"\n",
    "_CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def _resize_bicubic(images, size):\n",
    "    images = torch.from_numpy(images.transpose(0, 3, 1, 2))\n",
    "    images = torch.nn.functional.interpolate(images, size=(size, size), mode=\"bicubic\")\n",
    "    images = images.permute(0, 2, 3, 1).numpy()\n",
    "    return images\n",
    "\n",
    "\n",
    "class ClipEmbeddingModel:\n",
    "    \"\"\"CLIP image embedding calculator.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(_CLIP_MODEL_NAME)\n",
    "\n",
    "        self._model = CLIPVisionModelWithProjection.from_pretrained(_CLIP_MODEL_NAME).eval()\n",
    "        if _CUDA_AVAILABLE:\n",
    "            self._model = self._model.cuda()\n",
    "\n",
    "        self.input_image_size = self.image_processor.crop_size[\"height\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, images):\n",
    "        \"\"\"Computes CLIP embeddings for the given images.\n",
    "\n",
    "        Args:\n",
    "          images: An image array of shape (batch_size, height, width, 3). Values are\n",
    "            in range [0, 1].\n",
    "\n",
    "        Returns:\n",
    "          Embedding array of shape (batch_size, embedding_width).\n",
    "        \"\"\"\n",
    "\n",
    "        images = _resize_bicubic(images, self.input_image_size)\n",
    "        inputs = self.image_processor(\n",
    "            images=images,\n",
    "            do_normalize=True,\n",
    "            do_center_crop=False,\n",
    "            do_resize=False,\n",
    "            do_rescale=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        if _CUDA_AVAILABLE:\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        image_embs = self._model(**inputs).image_embeds.cpu()\n",
    "        image_embs /= torch.linalg.norm(image_embs, axis=-1, keepdims=True)\n",
    "        return image_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"IO utilities.\"\"\"\n",
    "\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class CMMDDataset(Dataset):\n",
    "    def __init__(self, path, reshape_to, max_count=-1):\n",
    "        self.path = path\n",
    "        self.reshape_to = reshape_to\n",
    "\n",
    "        self.max_count = max_count\n",
    "        img_path_list = self._get_image_list()\n",
    "        if max_count > 0:\n",
    "            img_path_list = img_path_list[:max_count]\n",
    "        self.img_path_list = img_path_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "    def _get_image_list(self):\n",
    "        ext_list = [\"png\", \"jpg\", \"jpeg\"]\n",
    "        image_list = []\n",
    "        for ext in ext_list:\n",
    "            image_list.extend(glob.glob(f\"{self.path}/*{ext}\"))\n",
    "            image_list.extend(glob.glob(f\"{self.path}/*.{ext.upper()}\"))\n",
    "        # Sort the list to ensure a deterministic output.\n",
    "        image_list.sort()\n",
    "        return image_list\n",
    "\n",
    "    def _center_crop_and_resize(self, im, size):\n",
    "        w, h = im.size\n",
    "        l = min(w, h)\n",
    "        top = (h - l) // 2\n",
    "        left = (w - l) // 2\n",
    "        box = (left, top, left + l, top + l)\n",
    "        im = im.crop(box)\n",
    "        # Note that the following performs anti-aliasing as well.\n",
    "        return im.resize((size, size), resample=Image.BICUBIC)  # pytype: disable=module-attr\n",
    "\n",
    "    def _read_image(self, path, size):\n",
    "        im = Image.open(path)\n",
    "        if size > 0:\n",
    "            im = self._center_crop_and_resize(im, size)\n",
    "        return np.asarray(im).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_path_list[idx]\n",
    "\n",
    "        x = self._read_image(img_path, self.reshape_to)\n",
    "        if x.ndim == 3:\n",
    "            return x\n",
    "        elif x.ndim == 2:\n",
    "            # Convert grayscale to RGB by duplicating the channel dimension.\n",
    "            return np.tile(x[Ellipsis, np.newaxis], (1, 1, 3))\n",
    "\n",
    "\n",
    "def compute_embeddings_for_dir(\n",
    "    img_dir,\n",
    "    embedding_model,\n",
    "    batch_size,\n",
    "    max_count=-1,\n",
    "):\n",
    "    \"\"\"Computes embeddings for the images in the given directory.\n",
    "\n",
    "    This drops the remainder of the images after batching with the provided\n",
    "    batch_size to enable efficient computation on TPUs. This usually does not\n",
    "    affect results assuming we have a large number of images in the directory.\n",
    "\n",
    "    Args:\n",
    "      img_dir: Directory containing .jpg or .png image files.\n",
    "      embedding_model: The embedding model to use.\n",
    "      batch_size: Batch size for the embedding model inference.\n",
    "      max_count: Max number of images in the directory to use.\n",
    "\n",
    "    Returns:\n",
    "      Computed embeddings of shape (num_images, embedding_dim).\n",
    "    \"\"\"\n",
    "    dataset = CMMDDataset(img_dir, reshape_to=embedding_model.input_image_size, max_count=max_count)\n",
    "    count = len(dataset)\n",
    "    print(f\"Calculating embeddings for {count} images from {img_dir}.\")\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    all_embs = []\n",
    "    for batch in tqdm.tqdm(dataloader, total=count // batch_size):\n",
    "        image_batch = batch.numpy()\n",
    "\n",
    "        # Normalize to the [0, 1] range.\n",
    "        image_batch = image_batch / 255.0\n",
    "\n",
    "        if np.min(image_batch) < 0 or np.max(image_batch) > 1:\n",
    "            raise ValueError(\n",
    "                \"Image values are expected to be in [0, 1]. Found:\" f\" [{np.min(image_batch)}, {np.max(image_batch)}].\"\n",
    "            )\n",
    "\n",
    "        # Compute the embeddings using a pmapped function.\n",
    "        embs = np.asarray(\n",
    "            embedding_model.embed(image_batch)\n",
    "        )  # The output has shape (num_devices, batch_size, embedding_dim).\n",
    "        all_embs.append(embs)\n",
    "\n",
    "    all_embs = np.concatenate(all_embs, axis=0)\n",
    "\n",
    "    return all_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jocke\\OneDrive\\Skrivbord\\Skola\\Delft_VT25\\ML Bayes - EE4685\\GAN\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"The main entry point for the CMMD calculation.\"\"\"\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import distance\n",
    "import embedding\n",
    "import io_util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "_BATCH_SIZE = flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for embedding generation.\")\n",
    "_MAX_COUNT = flags.DEFINE_integer(\"max_count\", -1, \"Maximum number of images to read from each directory.\")\n",
    "_REF_EMBED_FILE = flags.DEFINE_string(\n",
    "    \"ref_embed_file\", None, \"Path to the pre-computed embedding file for the reference images.\"\n",
    ")\n",
    "\n",
    "\n",
    "def compute_cmmd(ref_dir, eval_dir, ref_embed_file=None, batch_size=32, max_count=-1):\n",
    "    \"\"\"Calculates the CMMD distance between reference and eval image sets.\n",
    "\n",
    "    Args:\n",
    "      ref_dir: Path to the directory containing reference images.\n",
    "      eval_dir: Path to the directory containing images to be evaluated.\n",
    "      ref_embed_file: Path to the pre-computed embedding file for the reference images.\n",
    "      batch_size: Batch size used in the CLIP embedding calculation.\n",
    "      max_count: Maximum number of images to use from each directory. A\n",
    "        non-positive value reads all images available except for the images\n",
    "        dropped due to batching.\n",
    "\n",
    "    Returns:\n",
    "      The CMMD value between the image sets.\n",
    "    \"\"\"\n",
    "    if ref_dir and ref_embed_file:\n",
    "        raise ValueError(\"`ref_dir` and `ref_embed_file` both cannot be set at the same time.\")\n",
    "    embedding_model = embedding.ClipEmbeddingModel()\n",
    "    if ref_embed_file is not None:\n",
    "        ref_embs = np.load(ref_embed_file).astype(\"float32\")\n",
    "    else:\n",
    "        ref_embs = io_util.compute_embeddings_for_dir(ref_dir, embedding_model, batch_size, max_count).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "    eval_embs = io_util.compute_embeddings_for_dir(eval_dir, embedding_model, batch_size, max_count).astype(\"float32\")\n",
    "    val = distance.mmd(ref_embs, eval_embs)\n",
    "    return val.numpy()\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    if len(argv) != 3:\n",
    "        raise app.UsageError(\"Too few/too many command-line arguments.\")\n",
    "    _, dir1, dir2 = argv\n",
    "    print(\n",
    "        \"The CMMD value is: \"\n",
    "        f\" {compute_cmmd(dir1, dir2, _REF_EMBED_FILE.value, _BATCH_SIZE.value, _MAX_COUNT.value):.3f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Main\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We define all relevant discriminators, generators, optimizers, losses etc. and run the training for an `NUM_EPOCHS` amount of times. If we set `SAVE_MODEL` to `true`, we also keep a copy of every epoch (for example, for debugging, or to take a previous version in case the model started to overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    disc_P = Discriminator(in_channels=3).to(DEVICE)\n",
    "    disc_M = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen_P = Generator(img_channels=3, num_residuals=9). to (DEVICE)\n",
    "    gen_M = Generator(img_channels=3, num_residuals=9). to (DEVICE)\n",
    "    opt_disc = optim.Adam(\n",
    "        list(disc_P.parameters()) + list(disc_M.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    opt_gen = optim.Adam(\n",
    "        list(gen_P.parameters()) + list(gen_M.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    L1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    # These checkpoint files allow the training process to resume from where it left off, without starting over from scratch.\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_P,\n",
    "            gen_P,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_M,\n",
    "            gen_M,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_P,\n",
    "            disc_P,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_M,\n",
    "            disc_M,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "    \n",
    "    train_dataset = PhotoMonetDataset(\n",
    "        root_photo=TRAIN_DIR + \"/Photo\",\n",
    "        root_monet=TRAIN_DIR + \"/Monet\",\n",
    "        transform=transforms,\n",
    "    )\n",
    "    val_dataset = PhotoMonetDataset(\n",
    "         root_photo=VAL_DIR + \"/Photo\",\n",
    "         root_monet=VAL_DIR + \"/Monet\",\n",
    "         transform=transforms,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    g_scaler = torch.amp.GradScaler('cuda')\n",
    "    d_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    G_loss = []\n",
    "    D_loss = []\n",
    "    CMMD_Monet = []\n",
    "    CMMD_Photo = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "\n",
    "        print(\"Training...\")\n",
    "        train_fn(\n",
    "            disc_P,\n",
    "            disc_M,\n",
    "            gen_P,\n",
    "            gen_M,\n",
    "            train_loader,\n",
    "            opt_disc,\n",
    "            opt_gen,\n",
    "            L1,\n",
    "            mse,\n",
    "            d_scaler,\n",
    "            g_scaler,\n",
    "            epoch\n",
    "        )\n",
    "\n",
    "        print(\"Validation...\")\n",
    "        new_d_loss, new_g_loss = validate_fn(\n",
    "            disc_P,\n",
    "            disc_M,\n",
    "            gen_P,\n",
    "            gen_M,\n",
    "            val_loader,\n",
    "            L1,\n",
    "            mse,\n",
    "        )\n",
    "        \n",
    "        new_CMMD_Monet= compute_cmmd('Data/val/Monet', f'Output/Monet/{epoch}')\n",
    "        new_CMMD_Photo= compute_cmmd('Data/val/Photo', f'Output/Photo/{epoch}')\n",
    "        \n",
    "\n",
    "        G_loss.append(new_g_loss)\n",
    "        D_loss.append(new_d_loss)\n",
    "\n",
    "        CMMD_Monet.append(new_CMMD_Monet)\n",
    "        CMMD_Photo.append(new_CMMD_Photo)\n",
    "\n",
    "        print(\"G loss: \", G_loss)\n",
    "        print(\"D loss:\", D_loss)\n",
    "        print(\"CMMD Monet:\", CMMD_Monet)\n",
    "        print(\"CMMD Photo:\", CMMD_Photo)\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen_P, opt_gen, filename=CHECKPOINT_GEN_P)\n",
    "            save_checkpoint(gen_M, opt_gen, filename=CHECKPOINT_GEN_M)\n",
    "            save_checkpoint(disc_P, opt_disc, filename=CHECKPOINT_CRITIC_P)\n",
    "            save_checkpoint(disc_M, opt_disc, filename=CHECKPOINT_CRITIC_M)\n",
    "    print(\"G loss: \", G_loss)\n",
    "    print(\"D loss: \", D_loss)\n",
    "    print(\"CMMD Monet:\", CMMD_Monet)\n",
    "    print(\"CMMD Photo:\", CMMD_Photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jocke\\OneDrive\\Skrivbord\\Skola\\Delft_VT25\\ML Bayes - EE4685\\bayes_venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6938 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
