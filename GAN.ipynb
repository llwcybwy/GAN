{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> \n",
    "<b>\n",
    "TITLE\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Description of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, install the following packages by running the code below.\n",
    "\n",
    "`pip install ipykernel, numpy, torch, albumentations, pillow, tqdm, torchvision`\n",
    "\n",
    "Run the following imports to load all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Config\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "The configurations for the GAN. Some variables such as the directories may have to be tweaked to fit your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Prefer to work on GPU if available\n",
    "TRAIN_DIR = \"Data/train\"\n",
    "VAL_DIR = \"Data/val\"\n",
    "BATCH_SIZE = 1 # i.e. SGD\n",
    "LEARNING_RATE = 1e-5\n",
    "LAMBDA_IDENTITY = 0.0 # weight for identity loss (photo->photo, Monet->Monet)\n",
    "LAMBDA_CYCLE = 10 # weight for cycle loss (photo->Monet, Monet->photo)\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 10\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN_P = \"genp.pth.tar\" # photos\n",
    "CHECKPOINT_GEN_M = \"genm.pth.tar\" # monet\n",
    "CHECKPOINT_CRITIC_P = \"criticp.pth.tar\"\n",
    "CHECKPOINT_CRITIC_M = \"criticm.pth.tar\"\n",
    "\n",
    "transforms = A.Compose( # Note that we only expand the dataset by flipping, and not by altering the colors or brightnesses (since these are intregral to the photos and Monet )\n",
    "    [\n",
    "        A.Resize(width=256, height=256),\n",
    "        A.HorizontalFlip(p=0.5), # Double dataset by flipping images\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    additional_targets={\"image0\": \"image\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Dataset\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We define the class of the dataset holding the photos and the Monet pictures, along with its generator, length, and item retrieval function. The retrieval finds the photo and Monet picture corresponding to the given index, finds the images from their respective paths, applies the given transformation on them (if any), and returns the two pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoMonetDataset(Dataset):\n",
    "    def __init__(self, root_photo, root_monet, transform=None):\n",
    "        self.root_photo = root_photo # dir to photos\n",
    "        self.root_monet = root_monet # dir to monet pictures\n",
    "        self.transform = transform\n",
    "\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.length_dataset = max(self.photo_len, self.monet_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        photo_img = self.photo_images[index % self.photo_len ] # preventing index errors\n",
    "        monet_img = self.monet_images[index % self.monet_len ]\n",
    "\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        print(f\"Trying to load: {monet_path}\")\n",
    "        if not os.path.exists(monet_path):\n",
    "            print(f\"âŒ ERROR: File not found -> {monet_path}\")\n",
    "        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=photo_img, image0=monet_img)\n",
    "            photo_img = augmentations[\"image\"]\n",
    "            monet_img = augmentations[\"image0\"]\n",
    "\n",
    "        return photo_img, monet_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Models\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "The GAN essentially consists of two models: a discriminator, which decides if an image is fake or not, and a generator, which generates fake images by converting an image from distribution (e.g. photo) to another (e.g. Monet painting). They work together to correctly train the model, see the below section on __Training__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "<b>\n",
    "Discriminator model\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "__Block__: Shorthand for a convolution block, consisting of a 2d convolution layer, a normalization, and a ReLU. Used to define the discriminatory model.\n",
    "\n",
    "__Discriminator__: Classifies images into \"Real\" and \"Fake\". It consists of blocks of iteratively smaller size, finally culminating in a one-dimensional output (0 is \"fake\" and 1 is \"true\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBlock\u001b[39;00m(nn.Module):   \u001b[38;5;66;03m# inheriting from nn. Module\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jocke\\OneDrive\\Skrivbord\\Skola\\Delft_VT25\\ML Bayes - EE4685\\ml_venv\\Lib\\site-packages\\torch\\__init__.py:274\u001b[39m\n\u001b[32m    270\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    272\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jocke\\OneDrive\\Skrivbord\\Skola\\Delft_VT25\\ML Bayes - EE4685\\ml_venv\\Lib\\site-packages\\torch\\__init__.py:250\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    248\u001b[39m is_loaded = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     res = \u001b[43mkernel32\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     last_error = ctypes.get_last_error()\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error != \u001b[32m126\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):   # inheriting from nn. Module\n",
    "    def __init__(self, in_channels, out_channels, stride ):\n",
    "        super().__init__()                          # is a way to call the constructor of a parent class in Python. It ensures that the parent class (nn.Module in PyTorch) is properly initialized when a child class is created.\n",
    "        self.conv =nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,4,stride,1,bias=True,padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64,128,256,512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(nn.Conv2d(in_channels,features[0],kernel_size=4,stride=2,padding=1, padding_mode=\"reflect\"),nn.LeakyReLU(0.2))\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride = 1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels,1,kernel_size=4,stride=1,padding=1, padding_mode=\"reflect\")) # output has dimension 1\n",
    "        self.model = nn.Sequential(*layers) # unwrapping the list\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x)) # normalize the output between 0 and 1\n",
    "\n",
    "    def test():\n",
    "        x = torch.randn((5, 3, 256, 256))\n",
    "        model = Discriminator(in_channels=3)\n",
    "        preds = model(x)\n",
    "        print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to test the discriminator\n",
    "Discriminator.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "<b>\n",
    "Generator model\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We wish to remove all \"unimportant features\" from the input to a bottleneck in the middle (downsampling), and then upscale it to the desired distribution, but keeping the most important features (upsampling). \n",
    "\n",
    "__ConvBlock__: Conv2d + ReLU if `down=True`, else the transposed (i.e., the same but in the other direction).\n",
    "\n",
    "__ResidualBlock__: Consists of two ConvBlock(down=True). Forward has a residual to avoid 0 gradients in deep networks.\n",
    "\n",
    "__Generator__: Consists of an initial convolution that processes the image input, followed by two down ConvBlocks boiling it down to its most important features, followed by some ResidualBlocks that process the features (without risking gradient losses due to deep networks). Finally, two up ConvBlocks are applied and a last convolution (i.e., the same as the beginning, without ReLU but using `tanh` in the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):    # Down and upsampling\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs) :#key word arguments\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels,out_channels,**kwargs), # either Conv2d (id down) or its transpose\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block= nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1, stride=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x+ self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "           [ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1)]\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range (num_residuals) ]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [ConvBlock(num_features*4, num_features*2,down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "             ConvBlock(num_features*2, num_features*1,down=False, kernel_size=3, stride=2, padding=1, output_padding=1)]\n",
    "        )\n",
    "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7,stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward (self,x):\n",
    "        x= self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x= layer(x)\n",
    "        x=self.residual_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x= layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "def generator_test():\n",
    "    img_channels = 3\n",
    "    img_size =256\n",
    "    x= torch.randn((2, img_channels, img_size, img_size))\n",
    "    gen = Generator(img_channels,64,9 )\n",
    "    print(gen(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to test the generator\n",
    "generator_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Utils\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Utility functions used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Training\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Here, we are first training the discriminant and then the generator. We are using the generator to get the loss of the discriminant, and the discriminant to train the generator. Note that for low batch sizes (e.g. 1), the difference between the generator/discriminant before and after the weight updating is quite small, which might be good since we ideally would like to update them \"at the same time\". This is of course impossible, but by using small batch sizes, we approximate this better.\n",
    "\n",
    "__Training of the discriminant__\n",
    "\n",
    "We first try our function on a true photo, then generate a fake photo from the Monet data and try our discriminant on this photo. We then compare the outputed values with the correct ones (1 and 0 respectively) using mse, and let the loss be the sum of the two. We then do the same for Monet and let the total loss be the average of the two losses. Finally, we update the weights based on the losses. This way, we train the discriminant to recognize both fake and true data using the generator. As the generator improves, so will the discriminant.\n",
    "\n",
    "__Training the generator__\n",
    "\n",
    "We wish to improve the generator in three regards:\n",
    "\n",
    "1. __Adverserial__: i.e., we want the generator to be able to fool the discriminant. Since the discriminant is improved in the previous iteration, we thus want to generate data and change the weights in the generator such that the discrimant believes it to be true, hence why we take the mse of the fake data wrt 1. Note that by \"moving away\" from the discriminator, we do not train a generator which can create images that the discriminator is tailored to recognize and converge pretty quickly, but we instead capitalize from our improved discriminator and use it to iteratively improve. # THIS MIGHT NOT BE SUPER CLEAR; BUT MY POINT IS THAT WE DON'T CREATE GENERATORS AND DISCRIMINATORS THAT FOUND A COMMON GROUND BUT PRACTICALLY USELESS, BUT RATHER THAT THE DISCRIMINATOR IS TRYING TO CATCH THE GENERATOR AND THE GENERATOR IS USING THE ADVANCING DISCRIMINATOR TO PUSH ITSELF FORWARDS\n",
    "2. __Cycle__: We wish our generator to be able to generate images following the same distribution as the target. To do this, we cycle over all images from the target distribution and take the $L^1$-loss to it wrt our generated image.\n",
    "3. __Identity__: To strengthen our model's capacity to generate images of the target distribution, we input an image of that distribution and similar to above take the $L^1$-loss.\n",
    "\n",
    "We combine all of these losses by summing them together, along with a weight factor to the cycle `LAMBDA_CYCLE` and identity losses `LAMBDA_IDENTITY` (and by extension also to the adversarial loss since lowering the weights of the others de-facto raises that of the adversarial one, and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(disc_P, disc_M, gen_P, gen_M, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n",
    "    loop = tqdm(loader, leave=True)    # progress bar\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        print(f\"Entering iteration {idx}\")\n",
    "        photo = photo.to(DEVICE)\n",
    "        monet = monet.to(DEVICE)\n",
    "\n",
    "        # Train Discriminators P and M.\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            fake_Photo = gen_P(monet) # generate a fake photo based off a Monet picture\n",
    "            D_P_real = disc_P(photo) # take a real photo and see if it is real or not\n",
    "            D_P_fake = disc_P(fake_Photo.detach()) # check whether the fake photo is classified as real or not\n",
    "            D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "            D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_fake))\n",
    "            D_P_loss = D_P_fake_loss+D_P_real_loss # loss based on how well it classified true and fake images\n",
    "\n",
    "            fake_Monet = gen_M(photo)\n",
    "            D_M_real = disc_P(monet)\n",
    "            D_M_fake = disc_P(fake_Monet.detach())\n",
    "            D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n",
    "            D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n",
    "            D_M_loss = D_M_fake_loss + D_M_real_loss\n",
    "\n",
    "            D_loss = (D_P_loss+D_M_loss)/2 # loss as average of Monet and photo\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update() # Update the discriminator based on the loss\n",
    "\n",
    "        # Train generators P and M\n",
    "        with ((torch.amp.autocast('cuda'))):\n",
    "            # Adverserial loss\n",
    "            D_P_fake = disc_P(fake_Photo) # whether the photo is fake or not (after updating)\n",
    "            D_M_fake = disc_M(fake_Monet)\n",
    "            Loss_G_M = mse(D_M_fake, torch.ones_like(D_M_fake))\n",
    "            Loss_G_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "            # Cycle loss\n",
    "            cycle_monet = gen_M(fake_Photo) # make a Monet out of a photo\n",
    "            cycle_photo = gen_P(fake_Monet)\n",
    "            cycle_monet_loss = l1(cycle_monet, monet)\n",
    "            cycle_photo_loss = l1(cycle_photo,photo)\n",
    "            # Identitiy loss\n",
    "            identity_photo= gen_P(photo) # generate photo out of a photo\n",
    "            identity_monet= gen_M(monet)\n",
    "            identity_monet_loss= l1(identity_photo,photo)\n",
    "            identity_photo_loss= l1(identity_monet,monet)\n",
    "            # Add all together\n",
    "            G_loss = (Loss_G_M+Loss_G_P\n",
    "            + cycle_monet_loss* LAMBDA_CYCLE\n",
    "            + cycle_photo_loss* LAMBDA_CYCLE\n",
    "            + identity_monet_loss * LAMBDA_IDENTITY\n",
    "            + identity_photo_loss * LAMBDA_IDENTITY) # use everything in loss, with weights\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update() # update based on total loss\n",
    "\n",
    "        if idx % 20 == 0:\n",
    "            print(\"hello\") # TODO: change this maybe\n",
    "            save_image(fake_Photo * 0.5 + 0.5, f\"saved_images/photo_{idx}.png\")\n",
    "            save_image(fake_Monet * 0.5 + 0.5, f\"saved_images/monet_{idx}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Main\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We define all relevant discriminators, generators, optimizers, losses etc. and run the training for an `NUM_EPOCHS` amount of times. If we set `SAVE_MODEL` to `true`, we also keep a copy of every epoch (for example, for debugging, or to take a previous version in case the model started to overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    disc_P = Discriminator(in_channels=3).to(DEVICE)\n",
    "    disc_M = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen_P = Generator(img_channels=3, num_residuals=9). to (DEVICE)\n",
    "    gen_M = Generator(img_channels=3, num_residuals=9). to (DEVICE)\n",
    "    opt_disc = optim.Adam(\n",
    "        list(disc_P.parameters()) + list(disc_M.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    opt_gen = optim.Adam(\n",
    "        list(gen_P.parameters()) + list(gen_M.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    L1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_P,\n",
    "            gen_P,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_M,\n",
    "            gen_M,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_P,\n",
    "            disc_P,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_M,\n",
    "            disc_M,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "    # These checkpoint files allow the training process to resume from where it left off, without starting over from scratch.\n",
    "    dataset = PhotoMonetDataset(\n",
    "        root_photo=TRAIN_DIR + \"/Photo\",\n",
    "        root_monet=TRAIN_DIR + \"/Monet\",\n",
    "        transform=transforms,\n",
    "    )\n",
    "    # val_dataset = PhotoMonetDataset(\n",
    "    #     root_photo=\"cyclegan_test/photo1\",\n",
    "    #     root_monet=\"cyclegan_test/monet1\",\n",
    "    #     transform=transforms,\n",
    "    # )\n",
    "    # val_loader = DataLoader(\n",
    "    #     val_dataset,\n",
    "    #     batch_size=1,\n",
    "    #     shuffle=False,\n",
    "    #     pin_memory=True,\n",
    "    # )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    g_scaler = torch.amp.GradScaler('cuda')\n",
    "    d_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(\n",
    "            disc_P,\n",
    "            disc_M,\n",
    "            gen_P,\n",
    "            gen_M,\n",
    "            loader,\n",
    "            opt_disc,\n",
    "            opt_gen,\n",
    "            L1,\n",
    "            mse,\n",
    "            d_scaler,\n",
    "            g_scaler,\n",
    "        )\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen_P, opt_gen, filename=CHECKPOINT_GEN_P)\n",
    "            save_checkpoint(gen_M, opt_gen, filename=CHECKPOINT_GEN_M)\n",
    "            save_checkpoint(disc_P, opt_disc, filename=CHECKPOINT_CRITIC_P)\n",
    "            save_checkpoint(disc_M, opt_disc, filename=CHECKPOINT_CRITIC_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
