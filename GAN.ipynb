{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> \n",
    "<b>\n",
    "TITLE\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "To start, install the following packages by running the code below.\n",
    "\n",
    "`pip install ipykernel, numpy, torch, albumentations, pillow, tqdm, torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Config\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "The configurations for the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Prefer to work on GPU if available\n",
    "TRAIN_DIR = \"Data/train\" # TODO: directly install from the internet\n",
    "VAL_DIR = \"Data/val\"\n",
    "BATCH_SIZE = 1 # Should probably be higher?\n",
    "LEARNING_RATE = 1e-5\n",
    "LAMBDA_IDENTITY = 0.0 \n",
    "LAMBDA_CYCLE = 10 \n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 10\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN_H = \"genh.pth.tar\"\n",
    "CHECKPOINT_GEN_Z = \"genz.pth.tar\"\n",
    "CHECKPOINT_CRITIC_H = \"critich.pth.tar\"\n",
    "CHECKPOINT_CRITIC_Z = \"criticz.pth.tar\"\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=256, height=256),\n",
    "        A.HorizontalFlip(p=0.5), # Double dataset by flipping images\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    additional_targets={\"image0\": \"image\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Dataset\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "We define the class of the dataset holding the photos and the Monet pictures, along with its generator, length, and item retrieval function. The retrieval finds the photo and Monet picture corresponding to the given index, finds the images from their respective paths, applies the given transformation on them (if any), and returns the two pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoMonetDataset(Dataset):\n",
    "    def __init__(self, root_photo, root_monet, transform=None):\n",
    "        self.root_photo = root_photo # dir to photos\n",
    "        self.root_monet = root_monet # dir to monet pictures\n",
    "        self.transform = transform\n",
    "\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.length_dataset = max(self.photo_len, self.monet_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        photo_img = self.photo_images[index % self.photo_len ] # preventing index errors\n",
    "        monet_img = self.monet_images[index % self.monet_len ]\n",
    "\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        print(f\"Trying to load: {monet_path}\")\n",
    "        if not os.path.exists(monet_path):\n",
    "            print(f\"âŒ ERROR: File not found -> {monet_path}\")\n",
    "        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n",
    "        monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=photo_img, image0=monet_img)\n",
    "            photo_img = augmentations[\"image\"]\n",
    "            monet_img = augmentations[\"image0\"]\n",
    "\n",
    "        return photo_img, monet_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Models\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Here we describe what is being done, and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "<b>\n",
    "Discriminator model\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "__Block__: Shorthand for a convolution block, consisting of a 2d convolution layer, a normalization, and a ReLU. Used to define the discriminatory model.\n",
    "\n",
    "__Discriminator__: Classifies images into \"Real\" and \"Fake\". Trained along side the other layers, and is itself used as a cost function to the other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):   # inheriting from nn. Module\n",
    "    def __init__(self, in_channels, out_channels, stride ):\n",
    "        super().__init__()                          # is a way to call the constructor of a parent class in Python. It ensures that the parent class (nn.Module in PyTorch) is properly initialized when a child class is created.\n",
    "        self.conv =nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,4,stride,1,bias=True,padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64,128,256,512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(nn.Conv2d(in_channels,features[0],kernel_size=4,stride=2,padding=1, padding_mode=\"reflect\"),nn.LeakyReLU(0.2))\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride = 1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels,1,kernel_size=4,stride=1,padding=1, padding_mode=\"reflect\"))\n",
    "        self.model = nn.Sequential(*layers) # unwrapping the list\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))\n",
    "\n",
    "    def test():\n",
    "        x = torch.randn((5, 3, 256, 256))\n",
    "        model = Discriminator(in_channels=3)\n",
    "        preds = model(x)\n",
    "        print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to test the discriminator\n",
    "Discriminator.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "<b>\n",
    "Generator model\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "__ConvBlock__: WHAT IS MEANT WITH DOWN AND UPSAMPLING?? TODO: look up \"up and down\" in GANs\n",
    "\n",
    "__ResidualBlock__: Consists of two ConvBlock(down=True). Forward has a residual to avoid 0 gradients in deep networks.\n",
    "\n",
    "__Generator__: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):    # Down and upsampling\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs) :#key word arguments\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels,out_channels,**kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block= nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1, stride=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x+ self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "           [ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1)]\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range (num_residuals) ]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [ConvBlock(num_features*4, num_features*2,down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "             ConvBlock(num_features*2, num_features*1,down=False, kernel_size=3, stride=2, padding=1, output_padding=1)]\n",
    "        )\n",
    "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7,stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward (self,x):\n",
    "        x= self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x= layer(x)\n",
    "        x=self.residual_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x= layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "def generator_test():\n",
    "    img_channels = 3\n",
    "    img_size =256\n",
    "    x= torch.randn((2, img_channels, img_size, img_size))\n",
    "    gen = Generator(img_channels,64,9 )\n",
    "    print(gen(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to test the generator\n",
    "generator_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Utils\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Utility functions used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Training\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "What are we doing and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(disc_P, disc_M, gen_P, gen_M, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n",
    "    loop = tqdm(loader, leave=True)    # progress bar\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        print(f\"Entering iteration {idx}\")\n",
    "        photo = photo.to(DEVICE)\n",
    "        monet = monet.to(DEVICE)\n",
    "\n",
    "        # Train Discriminators H and Z.\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            fake_Photo= gen_P(monet)\n",
    "            D_P_real= disc_P(photo)\n",
    "            D_P_fake= disc_P(fake_Photo.detach())\n",
    "            D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "            D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_fake))\n",
    "            D_P_loss = D_P_fake_loss+D_P_real_loss\n",
    "\n",
    "            fake_Monet = gen_M(photo)\n",
    "            D_M_real = disc_P(monet)\n",
    "            D_M_fake = disc_P(fake_Monet.detach())\n",
    "            D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n",
    "            D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n",
    "            D_M_loss = D_M_fake_loss + D_M_real_loss\n",
    "\n",
    "            D_loss= (D_P_loss+D_M_loss)/2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train generators P and M\n",
    "        with ((torch.amp.autocast('cuda'))):\n",
    "            # Adverserial loss\n",
    "            D_P_fake = disc_P(fake_Photo)\n",
    "            D_M_fake=  disc_M(fake_Monet)\n",
    "            Loss_G_M= mse(D_M_fake, torch.ones_like(D_M_fake))\n",
    "            Loss_G_P= mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "            # Cycle loss\n",
    "            cycle_monet= gen_M(fake_Photo)\n",
    "            cycle_photo= gen_P(fake_Monet)\n",
    "            cycle_monet_loss= l1(cycle_monet, monet)\n",
    "            cycle_photo_loss= l1(cycle_photo,photo)\n",
    "            # Identitiy loss\n",
    "            identity_photo= gen_P(photo)\n",
    "            identity_monet= gen_M(monet)\n",
    "            identity_monet_loss= l1(identity_photo,photo)\n",
    "            identity_photo_loss= l1(identity_monet,monet)\n",
    "            # Add all together\n",
    "            G_loss = (Loss_G_M+Loss_G_P\n",
    "            + cycle_monet_loss* LAMBDA_CYCLE\n",
    "            + cycle_photo_loss* LAMBDA_CYCLE\n",
    "            + identity_monet_loss * LAMBDA_IDENTITY\n",
    "            + identity_photo_loss * LAMBDA_IDENTITY)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 20 == 0:\n",
    "            print(\"hello\") # TODO: change this maybe\n",
    "            save_image(fake_Photo * 0.5 + 0.5, f\"saved_images/photo_{idx}.png\")\n",
    "            save_image(fake_Monet * 0.5 + 0.5, f\"saved_images/monet_{idx}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "<b>\n",
    "Main\n",
    "</b>\n",
    "</font>\n",
    "\n",
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    disc_P = Discriminator(in_channels=3).to(DEVICE)\n",
    "    disc_M = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen_P = Generator(img_channels=3, num_residuals=9). to (DEVICE)\n",
    "    gen_M = Generator(img_channels=3, num_residuals=9). to (DEVICE)\n",
    "    opt_disc = optim.Adam(\n",
    "        list(disc_P.parameters()) + list(disc_M.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    opt_gen = optim.Adam(\n",
    "        list(gen_P.parameters()) + list(gen_M.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    L1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_P,\n",
    "            gen_P,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_M,\n",
    "            gen_M,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_P,\n",
    "            disc_P,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_M,\n",
    "            disc_M,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "    # These checkpoint files allow the training process to resume from where it left off, without starting over from scratch.\n",
    "    dataset = PhotoMonetDataset(\n",
    "        root_photo=TRAIN_DIR + \"/Photo\",\n",
    "        root_monet=TRAIN_DIR + \"/Monet\",\n",
    "        transform=transforms,\n",
    "    )\n",
    "    # val_dataset = PhotoMonetDataset(\n",
    "    #     root_photo=\"cyclegan_test/photo1\",\n",
    "    #     root_monet=\"cyclegan_test/monet1\",\n",
    "    #     transform=transforms,\n",
    "    # )\n",
    "    # val_loader = DataLoader(\n",
    "    #     val_dataset,\n",
    "    #     batch_size=1,\n",
    "    #     shuffle=False,\n",
    "    #     pin_memory=True,\n",
    "    # )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    g_scaler = torch.amp.GradScaler('cuda')\n",
    "    d_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(\n",
    "            disc_P,\n",
    "            disc_M,\n",
    "            gen_P,\n",
    "            gen_M,\n",
    "            loader,\n",
    "            opt_disc,\n",
    "            opt_gen,\n",
    "            L1,\n",
    "            mse,\n",
    "            d_scaler,\n",
    "            g_scaler,\n",
    "        )\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen_P, opt_gen, filename=CHECKPOINT_GEN_H)\n",
    "            save_checkpoint(gen_M, opt_gen, filename=CHECKPOINT_GEN_Z)\n",
    "            save_checkpoint(disc_P, opt_disc, filename=CHECKPOINT_CRITIC_H)\n",
    "            save_checkpoint(disc_M, opt_disc, filename=CHECKPOINT_CRITIC_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
